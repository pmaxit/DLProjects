# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/namegen.ipynb (unless otherwise specified).

__all__ = ['device', 'get_dataset', 'after_item', 'pad_input_chunk_new', 'train_from_scratch', 'load_from_checkpoint',
           'get_first_name_model']

# Cell
from typing import List, Optional
from fastai.text.all import *
from dotmap import DotMap
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
from torch import nn
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Cell
import sys
sys.path.insert(0,'/notebooks/Projects/Seq2Seq')

# Cell
from .model import RNN

# Cell
def get_dataset(names_list: List[str])-> Datasets:
    src_tfms = [lambda x: ['xxbos'] + list(x), Numericalize()]
    len_tfms = [lambda x: torch.tensor(len(x)+1, dtype=torch.int32)]
    #tgt_tfms = [lambda x: list(x)[1:]]

    dsrc = Datasets(names_list, tfms=[src_tfms, len_tfms], splits=RandomSplitter(valid_pct=0.1)(names_list))
    return dsrc

# Cell
@ItemTransform
def after_item(obj):
    return (obj[0][:], obj[0][1:], obj[1])

def pad_input_chunk_new(samples, n_inp=2,**kwargs):
    "Pad `samples` by adding padding by chunks of size `seq_len`"

    max_len = max([len(s[n]) for s in samples for n in range(n_inp)])
    padeds = [[pad_chunk(s[n],pad_len=max_len,**kwargs) for n in range(n_inp) ] for s in samples]

    return [(*p, *s[n_inp:]) for p, s in zip(padeds, samples)]

# Cell
def train_from_scratch(names_list: List[str], hparams:DotMap):
    dsrc = get_dataset(names_list)
    dls = dsrc.dataloaders(after_item=after_item, before_batch=pad_input_chunk_new, bs=32, n_inp=2)


    # get the model
    model = RNN(hparams, char2tensor = str(dict(dls.numericalize.o2i)), vocab=str(dls.numericalize.vocab))
    checkpoint_callback = ModelCheckpoint(
        dirpath = './checkpoints',
        filename='{epoch}',
        save_top_k=3,
        monitor='val_loss',
        mode='min'
    )
    trainer = pl.Trainer(fast_dev_run=False, auto_lr_find='learning_rate',gpus=1,
                    callbacks=[EarlyStopping(monitor='val_loss',patience=5), checkpoint_callback],
                    )


    trainer.fit(model, dls.train, dls.valid)

    return trainer

@patch
def gen_name(model:nn.Module, initial_char='A'):
    model = model.to(device)
    return model.generate(initial_char)

# Cell
def load_from_checkpoint(trainer:pl.Trainer, checkpoint:str)->pl.Trainer:
    # load from checkpoint
    trainer.model.load_from_checkpoint('example.ckpt')
    return trainer


def get_first_name_model(checkpoint:str="final_model.ckpt"):

    checkpoint = torch.load(checkpoint)
    # get the model

    model = RNN(checkpoint['hyper_parameters']['hp'],
                char2tensor = checkpoint['hyper_parameters']['char2tensor'],
            vocab= checkpoint['hyper_parameters']['vocab'])

    model.load_state_dict(checkpoint['state_dict'])

    model.eval()

    return model