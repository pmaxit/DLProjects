{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "younger-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export mllib.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desperate-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subjective-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "substantial-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "\n",
    "def mask_(matrices, maskval = 0.0, mask_diagonal=True):\n",
    "    \"\"\" mask out all values in the given batch of matrices where i <=j holds\"\". i < j if mask_diagonal is False\"\"\"\n",
    "    b, h, w = matrices.size()\n",
    "    \n",
    "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
    "    matrices[:, indices[0], indices[1]] = maskval\n",
    "    \n",
    "\n",
    "def d(tensor=None):\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "def contains_nan(tensor):\n",
    "    return bool((tensor != tensor).sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-penalty",
   "metadata": {},
   "source": [
    "# Self Attention Wide\n",
    "\n",
    "Self attention module resposible to create self attention among input elements. \n",
    "\n",
    "Parameters are \n",
    "\n",
    "a) Linear layers ( queries , keys , values )\n",
    "\n",
    "b) Unify heads ( emb* heads, heads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelefAttentionWide(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        '''\n",
    "            :param emb\n",
    "            :param heads\n",
    "            :param mask\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.emb = emb\n",
    "        self.heads = heds\n",
    "        self.mask = mask\n",
    "        \n",
    "        self.tokeys = nn.Linear(emb , emb*heads, bias = False)\n",
    "        self.toqueries = nn.Linear(emb, emb*heads, bias = False)\n",
    "        self.tovalues = nn.Linear(emb , emb*heads, bias=False)\n",
    "        \n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        \n",
    "        assert e == self.emb, f'Input embedding dim({e}) should match layer embedding dim ({self.emb})'\n",
    "        \n",
    "        keys = self.tokeys(x).view(b, t, h, e)\n",
    "        queries = self.toqueries(x).view(b, t, h ,e)\n",
    "        values = self.tovalues(x).view(b, t, h ,e)\n",
    "        \n",
    "        # compute scaled dot-product self-attention\n",
    "        \n",
    "        keys = keys.transpose(1, 2).contiguous().view(b*h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b*h, t, e)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h, t, e)\n",
    "        \n",
    "        queries = queries / (e**(1/4))\n",
    "        keys = keys / (e**(1/4))\n",
    "        \n",
    "        dot = torch.bmm(queries, keys.transpose(1,2))\n",
    "        \n",
    "        assert dot.size == (b*h, t, t)\n",
    "        \n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "            \n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        \n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h*e)\n",
    "        \n",
    "        return self.unifyheads(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-reply",
   "metadata": {},
   "source": [
    "# Narrow Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-laser",
   "metadata": {},
   "source": [
    "Narrow Self Attention is more computationally effective. It breaks the inputs into multiple parts and uses each of them to compute attention output. Finally it combines all attention outputs into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "under-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionNarrow(nn.Module):\n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
    "        \n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "        \n",
    "        s = emb // heads\n",
    "        # We will break embedding into `heads` chunks and feed each to different attention head\n",
    "        \n",
    "        self.tokeys= nn.Linear(s, s, bias=False)\n",
    "        self.toqueries = nn.Linear(s, s, bias=False)\n",
    "        self.tovalues = nn.Linear(s, s, bias=False)\n",
    "        \n",
    "        self.unifyheads = nn.Linear(heads * s, emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads()\n",
    "        \n",
    "        assert e == self.emb , f'Input dimension ({e}) should match layer embedding dim ({self.emb})'\n",
    "        \n",
    "        s = e//h\n",
    "        x = x.view(b, t, h, s)\n",
    "        \n",
    "        keys = self.tokeys(x)\n",
    "        queries = self.toqueries(x)\n",
    "        values = self.tovalues(x)\n",
    "        \n",
    "        assert keys.size() == (b, t, h, s)\n",
    "        assert queries.size() == (b , t, h, s)\n",
    "        assert values.size() == (b, t, h, s)\n",
    "        \n",
    "        \n",
    "        # compute scaled dot product self attention\n",
    "        keys = keys.transpose(1,2).contiguous().view(b*h,t, s)\n",
    "        queries = queries.transpose(1,2).contiguous().view(b*h, t, s)\n",
    "        values = values.transpose(1,2).contiguous().view(b*h, t, s)\n",
    "        \n",
    "        queries = queries / (e**(1/4))\n",
    "        keys = keys / (e**(1/4))\n",
    "        \n",
    "        dot = torch.bmm(queries, keys.transpose(1,2))\n",
    "        \n",
    "        assert dot.size() == (b*h, t, t)\n",
    "        \n",
    "        if self.mask:\n",
    "            mask_(dot, maskval=float('-inf'),mask_diagonal=False)\n",
    "        \n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        \n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "        \n",
    "        out = out.transpose(1,2).contiguous(b, t, s*h)\n",
    "        \n",
    "        return self.unifyheads(out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-repeat",
   "metadata": {},
   "source": [
    "Now comes the interesting part, how to use attention heads to form transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alien-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = SelfAttentionWide(emb, heads=heads, mask=mask)\n",
    "        \n",
    "        self.mask = mask\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult* emb, emb)\n",
    "        )\n",
    "        \n",
    "        self.do = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attended = self.attention(x)\n",
    "        \n",
    "        x = self.norm1(attended + x)\n",
    "        \n",
    "        x = self.do(x)\n",
    "        \n",
    "        feedforward = self.ff(x)\n",
    "        \n",
    "        x = self.norm2(feedforward + x)\n",
    "        \n",
    "        x = self.do(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-admission",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "respiratory-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secure-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTransformer(nn.Module):\n",
    "    \"\"\" \n",
    "        Transformer for generating text character by character\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, wide=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_tokens= num_tokens\n",
    "        self.token_embedding = nn.Embedding(embedding_dim = emb, num_embeddings = num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim = emb, num_embeddings = seq_length)\n",
    "        \n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(\n",
    "                    TransformerBlock(emb = emb, heads=heads, seq_length=seq_length, mask=True, wide=wide)\n",
    "                )\n",
    "            \n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        self.toprobs = nn.Linear(emb, num_tokens)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            param A (batch, sequence_length) integer tensor of token indices\n",
    "            return predicted log-probability vectors for each token based on preceding tokens\n",
    "        '''\n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "        \n",
    "        position = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b ,t, e)\n",
    "        x = tokens + positions\n",
    "        \n",
    "        x = self.tblocks(x)\n",
    "        \n",
    "        x = self.toprobs(x.view(b*t, e)).view(b , t, self.num_tokens)\n",
    "        \n",
    "        return F.log_softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-heading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
