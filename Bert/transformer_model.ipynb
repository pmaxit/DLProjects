{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organized-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fossil-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    ''' Encoder is made up of self attn and feed forward '''\n",
    "    \n",
    "    def __init__(self, size, self_attn, feed_forward, dropout, intermediate_layer_predictions=True, generator=None,\n",
    "                max_sequence_len = 512, force_prediction=False):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.add_positional_encoding = AddPositionalEncoding(size, max_sequence_len)\n",
    "        self.norm = self.sublayer[0].norm\n",
    "        \n",
    "        self.size = size\n",
    "        self.intermediate_layer_predictions = intermediate_layer_predictions\n",
    "        self.force_prediction = force_prediction\n",
    "        if intermediate_layer_predictions and self.training:\n",
    "            self.classifier = copy.deepcopy(generator)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.add_positional_encoding(x)\n",
    "        x = self.sublayer[0](x, lambda x: self.attn(x, x, x, mask))\n",
    "        x = self.sublayer[1](x, self.feed_forward)\n",
    "        if self.force_prediction or (self.intermediate_layer_predictions and self.training):\n",
    "            return x, self.classifier(self.norm(x))\n",
    "        \n",
    "        else:\n",
    "            return x, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "danish-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" Core Encoder is a stack of N layers\"\"\"\n",
    "    def __init__(self, layer, n_layers, intermediate_layer_predictions=True):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, n_layers)\n",
    "        self.layers[-1].force_prediction = True\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        self.intermediate_layer_predictions = intermediate_layer_predictions\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\" Pass the input ( and mask ) through each layer in turn. \"\"\"\n",
    "        intermediate_predictions = []\n",
    "        for layer in self.layers:\n",
    "            x, prediction = layer(x, mask)\n",
    "            intermediate_predictions.append(prediction)\n",
    "        return self.norm(x), intermediate_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "worst-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCrossEntropy(nn.Module):\n",
    "    def __init__(self, vocab_size, *args, **kwargs):\n",
    "        super(MultiLayerCrossEntropy, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, layer_outputs, target):\n",
    "        total_loss = torch.zeros(1, dtype=layer_outputs[-1].dtype, device= layer_outputs[-1].device)\n",
    "        n_layers_with_loss = 0\n",
    "        \n",
    "        for layer_output in layer_outputs:\n",
    "            if layer_output is not None:\n",
    "                # if True\n",
    "                if self.training:\n",
    "                    loss = self.cross_entropy(layer_output.view(-1, self.vocab_size).contiguous(), target)\n",
    "                    \n",
    "                else:\n",
    "                    # in evaluatin mode only the last prediction\n",
    "                    loss = self.cross_entropy(layer_output[:-1,:].contiguous(), target)\n",
    "                    \n",
    "                total_loss += loss\n",
    "                n_layers_with_loss += 1\n",
    "        \n",
    "        average_loss_of_all_layers = total_loss / n_layers_with_loss\n",
    "        final_layer_loss = loss\n",
    "        \n",
    "        return average_loss_of_all_layers , final_layer_loss\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unsigned-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextCharTransformer(nn.Module):\n",
    "    \"\"\" A standard next-character prediction model. Base for this and many other models \"\"\"\n",
    "    def __init__(self, vocab_size, n_layers=64, hidden_size=512, inner_linear = 2048, n_heads=8, dropout=0.55, tied=True,\n",
    "                max_sequence_len=512, intermediate_layer_predictions=True):\n",
    "        super().__init__()\n",
    "        attn = MultiHeadAttention(n_heads, hidden_size, dropout)\n",
    "        ff = PositionWiseFeedForward(hidden_size, inner_linear, dropout)\n",
    "        \n",
    "        generator = Generator(hidden_size, vocab_size)\n",
    "        self.encoder = Encoder(EncoderLayer(hidden_size, copy.deepcopy(attn), copy.deepcopy(ff),\n",
    "                                            dropout, intermediate_layer_predictions, generator,\n",
    "                                            max_sequence_len),\n",
    "                               n_layers, intermediate_layer_predictions)\n",
    "        self.embed = Embeddings(hidden_size, vocab_size)\n",
    "\n",
    "        self.criterion = MultiLayerCrossEntropy(vocab_size)\n",
    "\n",
    "        if tied:\n",
    "            self.generator.proj.weight = self.src_embed.lut.weight\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.intermediate_layer_predictions = intermediate_layer_predictions\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        \"\"\" Take in and process masked src and target sequences\"\"\"\n",
    "        src_emb = self.embed(src)\n",
    "        emb, intermediate_predictions = self.encoder(src_emb, mask)\n",
    "        return intermediate_predictions\n",
    "    \n",
    "    def update(self, training_percent):\n",
    "        \n",
    "        \"\"\" Stop using lossses from intermediate layer as functin of item in training \"\"\"\n",
    "        for i, layer in enumerate(self.encoder.layer[:-1]):\n",
    "            if training_percent > (i//2 *self.n_layers):\n",
    "                layer.intermediate_layer_predictions=False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-remedy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
