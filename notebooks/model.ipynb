{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exotic-daisy",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Here we define our pytorch model. Pytorch model we are targeting is LSTM based. It encodes past historical dependencies and helps to predict the future instances. Unlike RNN, it doesn't suffer much from Vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp seq2seq.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import rnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dotmap import DotMap\n",
    "from typing import Dict\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-peoples",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Here we define metrics for us to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def acc_cm(preds, labels, nb_clss):\n",
    "    \"\"\"Calculates all confusion matrix based metrics.\"\"\"\n",
    "    labels = labels.view(-1)\n",
    "    acc = (labels == preds).float().mean()\n",
    "\n",
    "    cm = torch.zeros((nb_clss, nb_clss), device=labels.device)\n",
    "    for label, pred in zip(labels, preds):\n",
    "        cm[label.long(), pred.long()] += 1\n",
    "\n",
    "    tp = cm.diagonal()[1:].sum()\n",
    "    fp = cm[:, 1:].sum() - tp\n",
    "    fn = cm[1:, :].sum() - tp\n",
    "    return (acc, tp, fp, fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-desert",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-secretariat",
   "metadata": {},
   "source": [
    "Here, model is defined as encoder / decoder architecture with LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_ID = 1\n",
    "\n",
    "class RNN(pl.LightningModule):\n",
    "    def __init__(self, hp:Dict, char2tensor, vocab, learning_rate=0.02):\n",
    "        super().__init__()\n",
    "        # char2tensor needs to be passed\n",
    "        self.hparams = hp\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = hp.num_layers\n",
    "        self.hidden_size = hp.hidden_size\n",
    "        self.output_size = hp.vocab_size\n",
    "        self.input_size = hp.vocab_size\n",
    "        self.embed_size = hp.embedding_size\n",
    "        self.char2tensor = eval(char2tensor)\n",
    "        self.vocab = eval(vocab)\n",
    "        self.dropout_p = 0.2\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embed_size,scale_grad_by_freq=True)\n",
    "        self.rnn = nn.LSTM(input_size = self.embed_size, hidden_size=self.hidden_size, dropout= self.dropout_p, num_layers = self.num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "\n",
    "        #self.criterion = nn.NLLLoss()\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        #self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.25)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param,gain=nn.init.calculate_gain('sigmoid'))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, input_seq, hidden_state):\n",
    "\n",
    "\n",
    "        embedding  = self.embedding(input_seq)\n",
    "        embedding = self.dropout(embedding)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        #output = F.log_softmax(output, -1)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        return h,c\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, tgt, lengths = batch\n",
    "\n",
    "        hidden_state = self.init_hidden(src.shape[0])\n",
    "        loss = 0\n",
    "        chunk_len = src.shape[1]\n",
    "\n",
    "        #for j in range(chunk_len):\n",
    "        #    output, hidden_state = self.forward(src[:,j],hidden_state)\n",
    "        #    output = output.reshape(output.shape[1]*output.shape[0],-1)\n",
    "        output, hidden_state = self.forward(src, hidden_state)\n",
    "        output = output.reshape(output.shape[1]*output.shape[0],-1)\n",
    "        loss = self.criterion(output, tgt.flatten())\n",
    "\n",
    "        self.log('loss',loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def custom_histogram_adder(self):\n",
    "        for name, params in self.named_parameters():\n",
    "            fig = plt.figure()\n",
    "            plt.hist(params.detach().cpu().numpy())\n",
    "            self.logger.experiment.log_image(name, fig)\n",
    "            plt.close('all')\n",
    "            #self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Funcion is called after every epoch is completed\n",
    "        # calculate average loss\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "\n",
    "        # logging histograms\n",
    "        self.custom_histogram_adder()\n",
    "        self.log('avg_loss', avg_loss)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, tgt, lengths = batch\n",
    "\n",
    "        hidden_state = self.init_hidden(src.shape[0])\n",
    "        loss = 0\n",
    "        chunk_len = src.shape[1]\n",
    "\n",
    "        #for j in range(chunk_len):\n",
    "        #    output, hidden_state = self.forward(src[:,j],hidden_state)\n",
    "        #    output = output.reshape(output.shape[1]*output.shape[0],-1)\n",
    "        output, hidden_state = self.forward(src, hidden_state)\n",
    "        output = output.reshape(output.shape[1]*output.shape[0],-1)\n",
    "        loss = self.criterion(output, tgt.flatten())\n",
    "\n",
    "        # metrics\n",
    "        preds = torch.argmax(output.data, dim=-1)\n",
    "        # preds = elementwise_apply(torch.argmax, output, -1)\n",
    "        (acc, tp, fp, fn) = acc_cm(preds, tgt.data, self.hparams.vocab_size)\n",
    "        self.log('val_loss', loss)\n",
    "        return {'val_loss': loss,'acc': acc, 'fp':fp, 'tp': tp }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # generate some names\n",
    "        names = ['A','B','R','KAR','TE','CHRI']\n",
    "        output = {n: self.generate(initial_char=n) for n in names}\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        acc = torch.stack([x['acc'] for x in outputs]).mean()\n",
    "        fp = torch.stack([x['fp'] for x in outputs]).mean()\n",
    "        tp = torch.stack([x['tp'] for x in outputs]).mean()\n",
    "\n",
    "        self.log('val_loss', avg_loss)\n",
    "        self.log('acc', acc)\n",
    "        self.log('tp',tp)\n",
    "        self.log('fp', fp)\n",
    "        print(output)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def num_training_steps(self) -> int:\n",
    "        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n",
    "        if self.trainer.max_steps:\n",
    "            return self.trainer.max_steps\n",
    "\n",
    "        limit_batches = self.trainer.limit_train_batches\n",
    "        batches = len(self.train_dataloader())\n",
    "        batches = min(batches, limit_batches) if isinstance(limit_batches, int) else int(limit_batches * batches)\n",
    "\n",
    "        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)\n",
    "        if self.trainer.tpu_cores:\n",
    "            num_devices = max(num_devices, self.trainer.tpu_cores)\n",
    "\n",
    "        effective_accum = self.trainer.accumulate_grad_batches * num_devices\n",
    "        return (batches // effective_accum) * self.trainer.max_epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        #optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams.lr)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.hparams.lr,  total_steps = self.num_training_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def generate(self, initial_char = 'A', predict_len = 15, temperature=0.85):\n",
    "        hidden, cell = self.init_hidden(batch_size = 1)\n",
    "\n",
    "        initial_input = TensorText([self.char2tensor[c] for c in initial_char ]).to(device)\n",
    "        predicted_str = initial_char\n",
    "\n",
    "        for p in range(len(initial_char)-1):\n",
    "            _, (hidden, cell) = self.forward(initial_input[p].view(1,1).to(device), (hidden, cell))\n",
    "\n",
    "        last_char = initial_input[-1]\n",
    "\n",
    "        for p in range(predict_len):\n",
    "            output, (hidden , cell) = self.forward(last_char.view(1,1).to(device), (hidden, cell))\n",
    "            # convert output to softmax\n",
    "            output = F.log_softmax(output, -1) # convert to softmax\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "            if top_char == PAD_ID:\n",
    "                # PADDING encountred stop\n",
    "                break\n",
    "\n",
    "            # convert back to string\n",
    "            predicted_char = self.vocab[top_char]\n",
    "            #predicted_char = all_chars[top_char]\n",
    "            predicted_str += predicted_char\n",
    "            last_char  = top_char\n",
    "\n",
    "        return predicted_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-scope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted model.ipynb.\n",
      "Converted namegen.ipynb.\n",
      "Converted run.ipynb.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-blast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
