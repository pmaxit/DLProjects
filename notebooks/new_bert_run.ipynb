{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "resistant-organ",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outdoor-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "african-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.new_bert import *\n",
    "from runs.callbacks import *\n",
    "from runs.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-explanation",
   "metadata": {},
   "source": [
    "# Bert Example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quick-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def random_examples(n_examples, n_largest):\n",
    "    letters = string.ascii_lowercase\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for i in range(n_examples):\n",
    "        l = random.choice(range(1,n_largest+1))\n",
    "        x = ''.join(random.choice(letters) for i in range(l))\n",
    "        y = ':'+ x[::-1]\n",
    "        yield x,y\n",
    "        \n",
    "data =[[x,y] for x,y in random_examples(10000,10)]\n",
    "raw_data={}\n",
    "raw_data['train'], raw_data['test'] = train_test_split(data, test_size=0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "structural-union",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6700/6700 [00:00<00:00, 318578.81lines/s]\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 314482.44lines/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = get_tokenizer(tokenizer=None), get_tokenizer(tokenizer=None) # split tokenizer\n",
    "tokenizer = list, list\n",
    "\n",
    "ds = ReversedString(data = raw_data, tokenizer=tokenizer,split_=('train','test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-hawaii",
   "metadata": {},
   "source": [
    "# Pytorch Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "widespread-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "from torch.optim.sgd import SGD\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "class LitTransformer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001, batch_size=4, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers=num_workers\n",
    "        \n",
    "        self.loss_crit = LabelSmoothingLoss2(ignore_value = 1, label_smoothing=0.1)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        # (N , 1, 1, src_len)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N , 1, trg_len, trg_len)\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        # get mask for src\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        return self.model.forward(src, src_mask, trg, trg_mask)\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        data = [[x,y] for x, y in random_examples(10000,10)]\n",
    "        self.raw_data={}\n",
    "        self.raw_data['train'], self.raw_data['test'] = train_test_split(data, test_size=0.33, random_state = 42)\n",
    "        \n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        tokenizer = list, list\n",
    "        reversed_train, reversed_test = ReversedString(data = raw_data, tokenizer=tokenizer)\n",
    "        \n",
    "        # save the vocab\n",
    "        self.src_vocab, self.trg_vocab = reversed_train.get_vocab()\n",
    "        \n",
    "        # define the model based on trg vocab. Note: We don't use src_vocab here.\n",
    "        self.model = make_model(len(self.trg_vocab), len(self.trg_vocab), \n",
    "                               N=4, d_model=128, d_ff=128, h=4, dropout=0.2)\n",
    "        \n",
    "        self.criterion = SimpleLossCompute(self.model.generator, self.loss_crit, None)\n",
    "\n",
    "        # train / val split\n",
    "        n = len(reversed_train)\n",
    "        p = int(0.8*n)\n",
    "        rerversed_train, reversed_val = random_split(reversed_train, [p, n-p])\n",
    "        \n",
    "        # asssign to use in dataloaders\n",
    "        self.train_ds = reversed_train\n",
    "        self.test_ds = reversed_test\n",
    "        self.val_ds = reversed_val\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "        # scheduler_warmup is chained with schduler_steplr\n",
    "        scheduler_steplr = StepLR(optim, step_size=10, gamma=0.1)\n",
    "        scheduler_warmup = GradualWarmupScheduler(optim, multiplier=1, total_epoch=5, after_scheduler=scheduler_steplr)\n",
    "    \n",
    "        return [optim],[scheduler_warmup]\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        src = src.permute(1,0)\n",
    "        trg = trg.permute(1,0)\n",
    "        \n",
    "        # pass through seq2seq model and get loss\n",
    "        out =  self.forward(src,trg[:,:-1])\n",
    "        loss = self.criterion(out, trg[:,1:])\n",
    "        self.log('loss', loss)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ret = self.training_step(batch, batch_idx)\n",
    "        self.log('val_loss', ret['loss'])\n",
    "        return {'val_loss': ret['loss']}\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dl = DataLoader(self.train_ds, self.batch_size,\n",
    "                          collate_fn=generate_batch_new, num_workers=self.num_workers)\n",
    "        return dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, self.batch_size,\n",
    "                          collate_fn=generate_batch_new,num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, self.batch_size,\n",
    "                          collate_fn=generate_batch_new,num_workers=self.num_workers)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-perth",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "flexible-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "logger = TensorBoardLogger('tb_logs', name='bert')\n",
    "model = LitTransformer()\n",
    "\n",
    "trainer = Trainer(fast_dev_run=False, progress_bar_refresh_rate=5, max_epochs=10,enable_pl_optimizer=False, \n",
    "                        callbacks=[\n",
    "                            ModelTestCallback(test='puneet'), \n",
    "                            LogHistogramCallback(),\n",
    "                            ModelCheckpoint(dirpath='.checkpoints/', monitor='val_loss')\n",
    "                        ], logger=logger, auto_lr_find=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "embedded-latitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 370204.28lines/s]\n",
      "\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 330081.71lines/s]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | loss_crit | LabelSmoothingLoss2 | 0     \n",
      "1 | model     | EncoderDecoder      | 1.1 M \n",
      "--------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.294     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1102f7a2e14d5d914f36d8a450c5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puneet/.virtualenvs/torch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a9b560a52841d991475339f2f0d87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puneet/.virtualenvs/torch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt = ModelTestCallback()\n",
    "# mt.on_fit_start(trainer, trainer.model)\n",
    "# mt.on_train_epoch_end(trainer,trainer.model, outputs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-vault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
