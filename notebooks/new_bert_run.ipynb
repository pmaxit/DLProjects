{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unknown-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alive-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "lovely-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.new_bert import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-worst",
   "metadata": {},
   "source": [
    "# Bert Example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dependent-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def random_examples(n_examples, n_largest):\n",
    "    letters = string.ascii_lowercase\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    for i in range(n_examples):\n",
    "        l = random.choice(range(1,n_largest+1))\n",
    "        x = ''.join(random.choice(letters) for i in range(l))\n",
    "        y = ':'+ x[::-1]\n",
    "        yield x,y\n",
    "        \n",
    "data =[[x,y] for x,y in random_examples(10000,10)]\n",
    "raw_data={}\n",
    "raw_data['train'], raw_data['test'] = train_test_split(data, test_size=0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "swedish-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.experimental.datasets.translation import *\n",
    "from torchtext.data.datasets_utils import _wrap_datasets\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "PAD_IDX = 1\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for (src, trg) in data_batch:\n",
    "        src_batch.append(src)\n",
    "        trg_batch.append(trg)\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value = PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value = PAD_IDX)\n",
    "    \n",
    "    # get mask for them as well\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "def ReversedString(data, tokenizer, split_=('train','test')):\n",
    "    # split the data into three parts\n",
    "    src_tokenizer, trg_tokenizer = tokenizer\n",
    "    src_text_vocab_transform = sequential_transforms(src_tokenizer)\n",
    "    trg_text_vocab_transform = sequential_transforms(trg_tokenizer)\n",
    "    \n",
    "    # build vocab only on training dataset\n",
    "    src_vocab = build_vocab(data['train'], src_text_vocab_transform, index=0)\n",
    "    trg_vocab = build_vocab(data['train'], trg_text_vocab_transform, index=1,)\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for key in split_:\n",
    "        src_text_transform = sequential_transforms(src_text_vocab_transform, \n",
    "                                                   vocab_func(trg_vocab), \n",
    "                                                   totensor(dtype=torch.long) )\n",
    "        trg_text_transform = sequential_transforms(trg_text_vocab_transform, \n",
    "                                                   vocab_func(trg_vocab), \n",
    "                                                   totensor(dtype=torch.long) )\n",
    "        \n",
    "        \n",
    "        datasets.append(TranslationDataset(data[key], (src_vocab, trg_vocab), (src_text_transform, trg_text_transform)))\n",
    "        \n",
    "    return _wrap_datasets(tuple(datasets), split_)\n",
    "\n",
    "# Here trick is to tie up the vocabulary between src and trg to make learning faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "wanted-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 428963.64lines/s]\n",
      "\n",
      "\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 362815.01lines/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = get_tokenizer(tokenizer=None), get_tokenizer(tokenizer=None) # split tokenizer\n",
    "tokenizer = list, list\n",
    "\n",
    "ds = ReversedString(data = raw_data, tokenizer=tokenizer,split_=('train','test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "stuck-maryland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([20,  5, 21, 10]), tensor([ 2, 10, 21,  5, 20]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "polar-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "from torch.optim.sgd import SGD\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "class LitTransformer(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001, batch_size=4, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.learning_rate=learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers=num_workers\n",
    "        \n",
    "        self.loss_crit = LabelSmoothingLoss2(ignore_value = 1, label_smoothing=0.1)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "        # (N , 1, 1, src_len)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N , 1, trg_len, trg_len)\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        # get mask for src\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        return self.model.forward(src, src_mask, trg, trg_mask)\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        data = [[x,y] for x, y in random_examples(10000,10)]\n",
    "        self.raw_data={}\n",
    "        self.raw_data['train'], self.raw_data['test'] = train_test_split(data, test_size=0.33, random_state = 42)\n",
    "        \n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        tokenizer = list, list\n",
    "        reversed_train, reversed_test = ReversedString(data = raw_data, tokenizer=tokenizer)\n",
    "        \n",
    "        # save the vocab\n",
    "        self.src_vocab, self.trg_vocab = reversed_train.get_vocab()\n",
    "        \n",
    "        # define the model based on trg vocab. Note: We don't use src_vocab here.\n",
    "        self.model = make_model(len(self.trg_vocab), len(self.trg_vocab), \n",
    "                               N=4, d_model=128, d_ff=128, h=4, dropout=0.2)\n",
    "        \n",
    "        self.criterion = SimpleLossCompute(self.model.generator, self.loss_crit, None)\n",
    "\n",
    "        # train / val split\n",
    "        n = len(reversed_train)\n",
    "        p = int(0.8*n)\n",
    "        rerversed_train, reversed_val = random_split(reversed_train, [p, n-p])\n",
    "        \n",
    "        # asssign to use in dataloaders\n",
    "        self.train_ds = reversed_train\n",
    "        self.test_ds = reversed_test\n",
    "        self.val_ds = reversed_val\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "        # scheduler_warmup is chained with schduler_steplr\n",
    "        scheduler_steplr = StepLR(optim, step_size=10, gamma=0.1)\n",
    "        scheduler_warmup = GradualWarmupScheduler(optim, multiplier=1, total_epoch=5, after_scheduler=scheduler_steplr)\n",
    "    \n",
    "        return [optim],[scheduler_warmup]\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        src = src.permute(1,0)\n",
    "        trg = trg.permute(1,0)\n",
    "        \n",
    "        # pass through seq2seq model and get loss\n",
    "        out =  self.forward(src,trg[:,:-1])\n",
    "        loss = self.criterion(out, trg[:,1:])\n",
    "        self.log('loss', loss)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ret = self.training_step(batch, batch_idx)\n",
    "        self.log('val_loss', ret['loss'])\n",
    "        return {'val_loss': ret['loss']}\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dl = DataLoader(self.train_ds, self.batch_size,\n",
    "                          collate_fn=generate_batch, num_workers=self.num_workers)\n",
    "        return dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, self.batch_size,\n",
    "                          collate_fn=generate_batch,num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, self.batch_size,\n",
    "                          collate_fn=generate_batch,num_workers=self.num_workers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "future-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogHistogramCallback(pl.Callback):\n",
    "    def __init__(self, patience=25):\n",
    "        self.patience = patience\n",
    "        \n",
    "    def on_after_backward(self, trainer, pl_module):\n",
    "        if trainer.global_step % self.patience == 0:\n",
    "            for k, v in pl_module.named_parameters():\n",
    "                trainer.logger.experiment.add_histogram(tag=k, values=v.grad, global_step = trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "qualified-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTestCallback(pl.Callback):\n",
    "    def __init__(self, max_len=10, test ='puneet'):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.test_sentence = test\n",
    "    \n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        # called when trainer setup is done.. model initiatlization has not happened yet\n",
    "        self.transforms = pl_module.train_ds.transforms\n",
    "        self.vocabs = pl_module.train_ds.get_vocab()\n",
    "    \n",
    "    def on_train_epoch_end(self, trainer, pl_module, outputs):\n",
    "        # take a random sentence and convert\n",
    "        # apply src transforms on the text\n",
    "        # here output contains the dictionary coming from training_step\n",
    "\n",
    "        self.trg_vocab = self.vocabs[1]\n",
    "        src_tensor = self.transforms[0](self.test_sentence).unsqueeze(0)\n",
    "        src_mask = pl_module.make_src_mask(src_tensor)       # N X 1 X 6 X 6\n",
    "        \n",
    "        # output tensor\n",
    "        out = \":\"        # initial target\n",
    "        out_tensor = self.transforms[1](out) \n",
    "        with torch.no_grad():\n",
    "            enc_src = pl_module.model.encode(src_tensor, src_mask)\n",
    "            \n",
    "        trg_indices = [2]\n",
    "        for i in range(self.max_len):\n",
    "            trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
    "            trg_mask = pl_module.make_trg_mask(trg_tensor)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                output = pl_module.model.decode(enc_src, src_mask, trg_tensor, trg_mask)\n",
    "                output = pl_module.model.generator(output)\n",
    "\n",
    "                pred_token = output.argmax(2)[:,-1].item()\n",
    "                trg_indices.append(pred_token)\n",
    "                \n",
    "                if pred_token == PAD_IDX:\n",
    "                    break\n",
    "\n",
    "        trg_tokens = [self.trg_vocab.itos[i] for i in trg_indices]\n",
    "        decode_string = ''.join(trg_tokens)\n",
    "        print('decoded input : {} -> output: {} '.format(self.test_sentence, decode_string))\n",
    "        trainer.logger.experiment.add_text('decodes', decode_string, trainer.current_epoch)\n",
    "        \n",
    "        return trg_tokens[1:]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-happening",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "minute-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "logger = TensorBoardLogger('tb_logs', name='bert')\n",
    "model = LitTransformer()\n",
    "\n",
    "trainer = Trainer(fast_dev_run=False, progress_bar_refresh_rate=5, max_epochs=10,enable_pl_optimizer=False, \n",
    "                        callbacks=[\n",
    "                            ModelTestCallback(test='puneet'), \n",
    "                            LogHistogramCallback(),\n",
    "                            ModelCheckpoint(dirpath='.checkpoints/', monitor='val_loss')\n",
    "                        ], logger=logger, auto_lr_find=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "twelve-symphony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 385749.30lines/s]\n",
      "\n",
      "100%|██████████| 6700/6700 [00:00<00:00, 371992.97lines/s]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | loss_crit | LabelSmoothingLoss2 | 0     \n",
      "1 | model     | EncoderDecoder      | 1.1 M \n",
      "--------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.294     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0d8f391845492d9764fc2b8168a0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53228809d2a2429d8d0bd29b8f6bd0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "plain-jacket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded input : puneet -> output: :teenupuppp \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'e', 'e', 'n', 'u', 'p', 'u', 'p', 'p', 'p']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt = ModelTestCallback()\n",
    "mt.on_fit_start(trainer, trainer.model)\n",
    "mt.on_train_epoch_end(trainer,trainer.model, outputs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-dayton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
