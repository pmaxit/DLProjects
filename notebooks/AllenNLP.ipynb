{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "controlling-spectacular",
   "metadata": {},
   "source": [
    "# Allen NLP\n",
    "\n",
    "Tutorial on how to use AllenNLP binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "undefined-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "import json\n",
    "from overrides import overrides\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field, ListField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer\n",
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norwegian-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-stroke",
   "metadata": {},
   "source": [
    "# Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-compression",
   "metadata": {},
   "source": [
    "A `Field` contains one piece of data for one example that is passed through your model. `Fields` get converted to tensors in a model, either as an input or an output, after being converted to IDs, batched and padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-angle",
   "metadata": {},
   "source": [
    "There are many types of fields in AllenNLP on the type of data that they represent. Among them, the most important is `TextFields`, which represents a piece of tokenized text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-concert",
   "metadata": {},
   "source": [
    "Others commonly used fields include:\n",
    "\n",
    "* `LabelField`\n",
    "* `MultiLabelField`\n",
    "* `SequenceLabelField`\n",
    "* `SpanField`\n",
    "* `ListField`\n",
    "* `ArrayField`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "northern-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polyphonic-vaccine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextField of length 5 with text: \n",
      " \t\t[the, best, movie, ever, !]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "LabelField with label: pos in namespace: 'labels'.\n",
      "SequenceLabelField of length 5 with labels:\n",
      " \t\t['DET', 'ADJ', 'NOUN', 'ADV', 'PUNKT']\n",
      " \t\tin namespace: 'labels'.\n"
     ]
    }
   ],
   "source": [
    "tokens = [Token(\"the\"), Token('best'), Token('movie'), Token('ever'), Token('!')]\n",
    "token_indexers: Dict[str, TokenIndexer]  = {'tokens': SingleIdTokenIndexer()}\n",
    "text_field = TextField(tokens, token_indexers = token_indexers)\n",
    "\n",
    "label_field = LabelField(\"pos\")\n",
    "\n",
    "sequence_label_field = SequenceLabelField([\"DET\", \"ADJ\", \"NOUN\",\"ADV\",\"PUNKT\"], text_field)\n",
    "\n",
    "print(text_field)\n",
    "print(label_field)\n",
    "print(sequence_label_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-power",
   "metadata": {},
   "source": [
    "# Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-trance",
   "metadata": {},
   "source": [
    "Instance is a collection of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "greatest-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = {\n",
    "    'tokens': text_field,\n",
    "    'label' : label_field\n",
    "}\n",
    "instance = Instance(fields)\n",
    "instance.add_field('label_seq', sequence_label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "artificial-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t tokens: TextField of length 5 with text: \n",
      " \t\t[the, best, movie, ever, !]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t label: LabelField with label: pos in namespace: 'labels'. \n",
      " \t label_seq: SequenceLabelField of length 5 with labels:\n",
      " \t\t['DET', 'ADJ', 'NOUN', 'ADV', 'PUNKT']\n",
      " \t\tin namespace: 'labels'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stupid-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary\n",
    "counter: Dict[str, Dict[str, int]] = defaultdict(Counter)\n",
    "instance.count_vocab_items(counter)\n",
    "vocab = Vocabulary(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "supreme-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': {'tokens': {'tokens': tensor([2, 3, 4, 5, 6])}}, 'label': tensor(0), 'label_seq': tensor([1, 2, 3, 4, 5])}\n"
     ]
    }
   ],
   "source": [
    "# convert all strings in all of the fields into integer IDs by calling index_fields()\n",
    "instance.index_fields(vocab)\n",
    "\n",
    "# instances know how to convert themselves into a dict of tensors.\n",
    "\n",
    "tensors = instance.as_tensor_dict()\n",
    "print(tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-venue",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "romantic-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in TextField:  [This, movie, was, awful, !]\n",
      "Label of labelfield negative\n"
     ]
    }
   ],
   "source": [
    "review = TextField(list(map(Token, [\"This\",\"movie\",\"was\",\"awful\",\"!\"])), token_indexers={'tokens': SingleIdTokenIndexer()})\n",
    "review_sentiment = LabelField('negative',label_namespace='tags')\n",
    "\n",
    "# Access the original strings and labels using the methods on fields\n",
    "print('Tokens in TextField: ',review.tokens)\n",
    "print('Label of labelfield',review_sentiment.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-tours",
   "metadata": {},
   "source": [
    "Once we've made our Fields. We need to pair them together to form an `instance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "particular-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in instances  {'review': <allennlp.data.fields.text_field.TextField object at 0x1425eb700>, 'label': <allennlp.data.fields.label_field.LabelField object at 0x1425eb2c0>}\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data import Instance\n",
    "instance1 = Instance({'review': review, 'label': review_sentiment})\n",
    "\n",
    "print('Fields in instances ', instance1.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# Represents each token with a single ID from a vocabulary.\n",
    "token_indexer: TokenIndexer = SingleIdTokenIndexer(namespace=\"token_vocab\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-stylus",
   "metadata": {},
   "source": [
    "### TokenIndexers\n",
    "\n",
    "Each TokenIndexers knows how to convert a `Token` into a representation that can be encoded by a corresponding piece of the model. \n",
    "\n",
    "- Mapping the token into vocabulary\n",
    "- breaking up the token into characters or wordpieces and representing the token by sequence of indexed characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unique-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from allennlp.data import Token, Vocabulary, TokenIndexer, Tokenizer\n",
    "from allennlp.data.fields import ListField, TextField\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "local-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "    \n",
    "token_indexer = SingleIdTokenIndexer(namespace = 'token_vocab')\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"This\", \"is\", \"some\", \"text\", \".\"], namespace=\"token_vocab\"\n",
    ")\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"T\", \"h\", \"i\", \"s\", \" \", \"o\", \"m\", \"e\", \"t\", \"x\", \".\"], namespace=\"character_vocab\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "still-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens  [this, is, some, text, .]\n"
     ]
    }
   ],
   "source": [
    "text=\"this is some text .\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print('Word tokens ', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "future-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = TextField(tokens, {\"tokens\": token_indexer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adapted-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the vocabulary \n",
    "text_field.index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "representative-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We typically batch things together when making tensors, which requires some\n",
    "# padding computation.  Don't worry too much about the padding for now.\n",
    "padding_lengths = text_field.get_padding_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "clean-ensemble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens___tokens': 5}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "automatic-princeton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With single id indexer: {'tokens': {'tokens': tensor([1, 3, 4, 5, 6])}}\n"
     ]
    }
   ],
   "source": [
    "tensor_dict = text_field.as_tensor(padding_lengths)\n",
    "# This output is pretty nested and might look complex.  The reason it is so\n",
    "# nested is that we need to (1) align each indexer with a corresponding\n",
    "# embedder in the model, and (2) pass a dictionary of arguments to the\n",
    "# embedder by name.  This will be more clear when we get to the embedder.\n",
    "print(\"With single id indexer:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexer = TokenCharactersIndexer(namespace=\"character_vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "flexible-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = TextField(tokens, {\"token_characters\": token_indexer})\n",
    "text_field.index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "actual-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_lengths = text_field.get_padding_lengths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "physical-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With token characters indexer: {'token_characters': {'token_characters': tensor([[10,  3,  4,  5],\n",
      "        [ 4,  5,  0,  0],\n",
      "        [ 5,  7,  8,  9],\n",
      "        [10,  9, 11, 10],\n",
      "        [12,  0,  0,  0]])}}\n"
     ]
    }
   ],
   "source": [
    "tensor_dict = text_field.as_tensor(padding_lengths)\n",
    "print(\"With token characters indexer:\", tensor_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "operational-dance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character tokens: [t, h, i, s,  , i, s,  , s, o, m, e,  , t, e, x, t,  , .]\n",
      "With single id indexer: {'token_characters': {'tokens': tensor([10,  3,  4,  5,  6,  4,  5,  6,  5,  7,  8,  9,  6, 10,  9, 11, 10,  6,\n",
      "        12])}}\n"
     ]
    }
   ],
   "source": [
    "# Splits text into characters (instead of words or wordpieces).\n",
    "tokenizer = CharacterTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Character tokens:\", tokens)\n",
    "\n",
    "# Represents each token (which is a character) as a single id from a vocabulary.\n",
    "token_indexer = SingleIdTokenIndexer(namespace=\"character_vocab\")\n",
    "\n",
    "text_field = TextField(tokens, {\"token_characters\": token_indexer})\n",
    "text_field.index(vocab)\n",
    "\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "\n",
    "tensor_dict = text_field.as_tensor(padding_lengths)\n",
    "print(\"With single id indexer:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-fifth",
   "metadata": {},
   "source": [
    "# combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "transsexual-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splits text into words (instead of wordpieces or characters).\n",
    "tokenizer: Tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "# Represents each token with both an id from a vocabulary and a sequence of\n",
    "# characters.\n",
    "token_indexers: Dict[str, TokenIndexer] = {\n",
    "    \"tokens\": SingleIdTokenIndexer(namespace=\"token_vocab\"),\n",
    "    \"token_characters\": TokenCharactersIndexer(namespace=\"character_vocab\"),\n",
    "}\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"This\", \"is\", \"some\", \"text\", \".\"], namespace=\"token_vocab\"\n",
    ")\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"T\", \"h\", \"i\", \"s\", \" \", \"o\", \"m\", \"e\", \"t\", \"x\", \".\"], namespace=\"character_vocab\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "central-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [This, is, some, text, .]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text .\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "neural-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The setup here is the same as what we saw above.\n",
    "text_field = TextField(tokens, token_indexers)\n",
    "text_field.index(vocab)\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "tensor_dict = text_field.as_tensor(padding_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "oriented-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined tensor dictionary: {'tokens': {'tokens': tensor([2, 3, 4, 5, 6])}, 'token_characters': {'token_characters': tensor([[ 2,  3,  4,  5],\n",
      "        [ 4,  5,  0,  0],\n",
      "        [ 5,  7,  8,  9],\n",
      "        [10,  9, 11, 10],\n",
      "        [12,  0,  0,  0]])}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined tensor dictionary:\", tensor_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "initial-easter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we split text into words with part-of-speech tags, using Spacy's POS tagger.\n",
    "# This will result in the `tag_` variable being set on each `Token` object, which\n",
    "# we will read in the indexer.\n",
    "tokenizer = SpacyTokenizer(pos_tags=True)\n",
    "vocab.add_tokens_to_namespace([\"DT\", \"VBZ\", \"NN\", \".\"], namespace=\"pos_tag_vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "flexible-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents each token with (1) an id from a vocabulary, (2) a sequence of\n",
    "# characters, and (3) part of speech tag ids.\n",
    "token_indexers = {\n",
    "    \"tokens\": SingleIdTokenIndexer(namespace=\"token_vocab\"),\n",
    "    \"token_characters\": TokenCharactersIndexer(namespace=\"character_vocab\"),\n",
    "    \"pos_tags\": SingleIdTokenIndexer(namespace=\"pos_tag_vocab\", feature_name=\"tag_\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "suspected-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy tokens: [This, is, some, text, .]\n",
      "POS tags: [(This, 'DT'), (is, 'VBZ'), (some, 'DT'), (text, 'NN'), (., '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Spacy tokens:\", tokens)\n",
    "print(\"POS tags:\", [(token, token.tag_) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hollywood-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = TextField(tokens, token_indexers)\n",
    "text_field.index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "tutorial-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_lengths = text_field.get_padding_lengths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "square-marketplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor dict with POS tags: {'tokens': {'tokens': tensor([2, 3, 4, 5, 6])}, 'token_characters': {'token_characters': tensor([[ 2,  3,  4,  5],\n",
      "        [ 4,  5,  0,  0],\n",
      "        [ 5,  7,  8,  9],\n",
      "        [10,  9, 11, 10],\n",
      "        [12,  0,  0,  0]])}, 'pos_tags': {'tokens': tensor([2, 3, 2, 4, 5])}}\n"
     ]
    }
   ],
   "source": [
    "tensor_dict = text_field.as_tensor(padding_lengths)\n",
    "print(\"Tensor dict with POS tags:\", tensor_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-prayer",
   "metadata": {},
   "source": [
    "# Text Field Embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-cholesterol",
   "metadata": {},
   "source": [
    "As a reminder, there are three main steps\n",
    "1. TOkenization (Text -> Tokens)\n",
    "2. Representing each token as some kind of ID using TextFields and TokenIndexers\n",
    "3. Embedding those IDs into vector space . TextFieldEmbedders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-bernard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
